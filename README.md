# Motivation
Studies of how human brain processes visual information (visual psychophysics, visual neuroscience) often use simple visual stimuli (lines, simple shapes, etc.). Although there are studies on more naturalistic stimuli such as face or natural scenes, they are pretty limited because it is difficult to manipulate, quantify and control the complex high-dimensional attributes of those stimuli. In this project, I aim to solve several of those problems using the state-of-the-arts deep learning methods. I will focus on face stimuli although this can be applied readily to other kinds of stimuli. Here's the summary of the problems:
* Create random look-like-real fake faces. In visual experiments, people either use real face images or synthetic images. For real face images, that does not meet the gold standard of randomization in experiment. For synthetic images, they often don't look like real.
* Manipulate a certain feature in the image (e.g. gender) while keeping all other features the same. Again, this is the gold standard of experimental design but it's really hard to do with the standard image processing techniques (think of Photoshop or even GIMP).
* Quantify the attributes in the images.

# Problem 1: Create random look-like-real face images
To generate fake face images, we use the [style-GAN](https://github.com/NVlabs/stylegan) pretrained network from Nvidia group. The details can be found in their [paper](https://arxiv.org/abs/1812.04948) and the github page. Here I just provide brief summary of key points. A key point in this GAN network is that the latent vector **z** (1x256) is not fed directly to the network but it first goes through a mapping network (fully-connected dense layers) that outputs an embedding representation **w** (18x256). Importantly, each row of **w** is fed directly to 18 layers of the generator. Because the generator was trained in such a way that the layers separately represent high-level to low-level features ([progressive GAN](https://arxiv.org/abs/1710.10196)), we can separately control for low- or high-level feature (more on this in the next section). Here is a sketch of style-GAN taken from the original paper: 
![](/figures/stylegan_base.PNG)

So basically, we just need to feed the netword a random vector **z** and it will output an image. Here are some *good* examples which look really impressive:
![](/figures/example_fake_face.PNG)

Also, note that some images generated by the networks have some artifacts so I had to carefully examine the images to make sure they are generally ok. Recently, the same group published an updated version that deals with those issues. If you are interested, you can find it [here](https://github.com/NVlabs/stylegan2).

# Problem 2: Manipulate a feature while keeping others constant
Ok now we have really good fake images, the next question is how can we manipulate certain feature while keeping others constant. Note that here for simplicity I just want to deal with either low or high-level changes. Here is a good example of what we want to achieve:
![](/figures/face_manipulation.PNG)

We start with an image and want to change only low-level features (skin tone, hair or eye color, etc.) while keeping high-level feature (e.g. identity) the same or vice versa. As mentioned in the previous section, the generator has separate layers that represent a hierarchy of high to low-level features. Therefore, if we have two images A and B and we want to mix a high-level feature of B into A, we can just "switch the high-level layers" while keeping other layers the same. Note that in the true implementation, we don't switch the layer itself but instead we switch the rows of embedding **w**.
![](/figures/stylegan_high.PNG)

We can do the same to manipulate low-level feature while keeping high-level features the same.
![](/figures/stylegan_low.PNG)

An implementation detail: there are 18 rows in the embedding **w** so in theory we have 18 levels of representation from high to low. However, first the layers come in pair (you can see in the image above that there are 2 style arrows in each box). So we are left with only 8 degrees of freedom. Moreover, I tried it out and actually most intermediate layers don't do much. So here I show the mixing result for only some combinations that I found work well. 
![](/figures/example_face_mixing.png)

The first column is the original image and the first row is the mixing image that I want to mix a low- or high-level feature into the original image. In the second row, I mix low-level features of the mixing image into the original image (layers 8-17). In the third and fourth rows, I mix high-level features (layers 2-3 or 0-3). We see that the mixing works pretty well. One thing I note is that it seems like the last 2 layers (0-1) control for the pose. So for high-level change, I decided to use only layers 2-3 and for low-level change I used layers 8-17.

# Problem 3: Quantify the attributes in the image
## Low-level attributes
![](/figures/example_pixelDiff_pca.png)
![](/figures/example_lowlevel_pca_more.png)
![](/figures/l2_lowChange.png)
![](/figures/histogram_low_high_flip.png)

## High-level attributes
![](/figures/facenet_architecture.PNG)
![](/figures/example_facenet_pca.png)
![](/figures/example_facenet_pca_more.png)
![](/figures/l2_highChange.png)




